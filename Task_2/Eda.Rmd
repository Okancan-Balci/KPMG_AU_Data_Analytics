---
output:
  html_document:
    theme: simplex
    highlight: pygments
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: true
    toc_depth: 4
    df_print: paged
    code_folding: show
---

```{r, include=FALSE}
library(tidyverse)
library(readxl)
library(GGally)
library(broom)
library(ggfortify)
library(skimr)
library(lubridate)
library(janitor) ## excel_numeric_to_date()



options(scipen = 20)

knitr::opts_chunk$set(warning = FALSE,
                      message = FALSE,
                      fig.align = "center",
                      fig.width = 8)

```



```{r, include=FALSE}
sheets <- readxl::excel_sheets("KPMG_VI_New_raw_data_update_final.xlsx")[-1]


for (sheet in sheets) {
    assign(as.character(sheet),
           read_xlsx("KPMG_VI_New_raw_data_update_final.xlsx", sheet = sheet, skip = 1))
}

```


# Data Cleaning

## Transactions

I don' think keeping cancelled orders is going to help us find lucrative customers. 

```{r}
Transactions <- Transactions %>% 
  filter(order_status != "Cancelled")
```

I am going to use `list_price` - `standard_cost` = `profit` as a candidate response variable. Also I want to try `profit_per_interaction` variable as a response as well. So I am removing the observations which miss `standard_cost`.

```{r}
Transactions <- Transactions %>% 
  filter(!is.na(standard_cost))
```

I am not going to remove observations with missing `online_order`values because their `list_price` and `standard_cost` values are intact.

This is the table that holds potential response variables. 

```{r}
response_table <- Transactions %>% 
  mutate(profit = list_price - standard_cost) %>% 
  group_by(customer_id) %>% 
  summarise(
    total_profit = sum(profit),
    n_transaction = n()
  ) %>% 
  mutate(profit_per_transaction = total_profit / n_transaction)

head(response_table)
```

The number of transactions seems like a nice candidate for Poisson Regression but Profits per Transaction has nice distribution except for very high values.

```{r}
response_table %>% 
  ggplot(aes(n_transaction))+
  geom_bar()

response_table %>% 
  ggplot(aes(total_profit))+
  geom_histogram(color = "white")

response_table %>% 
  ggplot(aes(profit_per_transaction))+
  geom_histogram(color = "white")
```

## Customer Demographic 

These tables hold our predictors.

There were some inconsistencies with the gender variable. For now I am only adjusting typos and "F", "M" type of errors. I am keeping gender "U" because it might have some systematic information. Though the amount of observations are very low. (I realized later that all observations of gender "U" were gone after I cleaned some other values because some variables were missing. No need to worry about them for now.)

```{r}
CustomerDemographic <- CustomerDemographic %>% 
  mutate(gender = ifelse(gender %in% c("F", "Femal"), "Female", gender)) %>%
  mutate(gender = ifelse(gender == "M", "Male", gender))
```

I am dropping some useless columns for modeling process. 

```{r}
CustomerDemographic <- CustomerDemographic %>% 
  select(-default, -first_name, -last_name, -deceased_indicator,)
```

Adjusting job titles. `job_industry_category` has more missing values than `job_title`. I am thinking of stripping latin numbers from job titles and using it as a predictor.

```{r}
## Missing Values for variable job_title
CustomerDemographic %>% 
  pull(job_title) %>% is.na() %>% sum()
## Missing Values for variable job_industry_category
CustomerDemographic %>% 
  filter(job_industry_category == "n/a") %>% nrow()
```

```{r}
CustomerDemographic <- CustomerDemographic %>% 
  mutate(job_title = str_remove(job_title, regex(" [IV]+"))) 
```

I am also removing observations with missing job titles.

```{r}
CustomerDemographic <- CustomerDemographic %>% 
  filter(!is.na(job_title))
```

Removing rest of the observations which have missing values for variables `DOB` and `tenure`. Luckily these observations are the same.

```{r}
CustomerDemographic <- CustomerDemographic %>% 
  filter(!is.na(tenure)) 
```

Parsing the `DOB` (the day of birth) correctly.

```{r}
CustomerDemographic <- CustomerDemographic %>% 
  mutate(DOB = convert_to_date(DOB))
```

There is one person who was born in 1843. I am removing that observation.

```{r}
CustomerDemographic %>% 
  slice_min(order_by = DOB, n = 5)
```

```{r}
CustomerDemographic <- CustomerDemographic %>% 
  filter(customer_id != 34)
```

I also want to engineer a customer age variable. Since all transactions were occurred in the year 2017, I will be calculating ages based on the middle of the 2017.

```{r}
CustomerDemographic <- CustomerDemographic %>% 
  mutate(age = as.integer((ymd("2017-06-01") - DOB) / 365.25))
```


```{r}
(nrow(CustomerDemographic) - 4000) / 4000
```

We only lost about %15 of our observations for this table. Not great but not terrible.

## Customer Address

The same states had different names in the data set. I am fixing that.

```{r}
CustomerAddress %>% count(state)

CustomerAddress <- CustomerAddress %>% 
  mutate(state = ifelse(state == "New South Wales", "NSW", state),
         state = ifelse(state == "Victoria", "VIC", state)) 
```

```{r}
CustomerAddress %>% count(state)
```


## Joining Customer Demographic and Customer Address Tables

I am joining these two tables to create a bigger predictor pool. Also removing the country column because every observation is from Australia.

```{r}
Customer_Joined <- CustomerDemographic %>% 
  inner_join(CustomerAddress, by = "customer_id") %>% 
  select(-country)
```

In the join process 2 observations were also lost. For now, this completes the data cleaning process for our training set.

## Cleaning the new data New Customer List Table

Filtering out irrelevant columns and creating a new data set.

```{r}
New_Costumers <- NewCustomerList %>% 
  select(any_of(names(Customer_Joined)))
```

Since I am going to use `day_of_birth` and `job_title` as predictors I am dropping observations with missing values.

```{r}
New_Costumers <- New_Costumers %>%
  drop_na()
```

Parsing `DOB` properly as a data variable.

```{r}
New_Costumers <- New_Costumers %>% 
  mutate(DOB = convert_to_date(DOB))
```

Adjusting `job_titles` and stripping latin numerics.

```{r}
New_Costumers <- New_Costumers %>% 
  mutate(job_title = str_remove(job_title, regex(" [IV]+"))) 
```

Creating an `age` variable for the new data set as well.

```{r}
New_Costumers <- New_Costumers %>% 
  mutate(age = as.integer((ymd("2017-06-01") - DOB) / 365.25)) 
```

Since I am thinking about using `job_title` variable as a predictor and it is a categorical variable I have to make sure that both train and prediction sets have the same categories.

```{r}
all(unique(Customer_Joined$job_title) %in% unique(New_Costumers$job_title))
```

Let's compare summary statistics for both train and prediction sets for the last time.

```{r}
New_Costumers %>% 
  skim()
```


```{r}
Customer_Joined %>%
  mutate(postcode = as.character(postcode)) %>% 
  skim()
```

Everything looks fine here.

## Merging Response variable and Predictor Variables into a single Table

```{r}
Customer_Joined %>% 
  inner_join(response_table, by = "customer_id") %>% nrow()
```

Seems like some of the customers didn't buy anything in 2017. I think leaving them out would be wrong and we would lose a good chunk of our data. After I left join these two tables I am also going to set profit values to zero for these consumers. This way Machine Learning algorithm may be able to learn what kind of consumers don't make any transactions.

```{r}
Customer_Joined <- Customer_Joined %>% 
  left_join(response_table, by = "customer_id") %>% 
  replace_na(list(total_profit = 0,
                  n_transaction = 0,
                  profit_per_transaction = 0))
```


# Exploratory Data Analysis

I made my best to clean the data now lets try to explore it.

```{r}
Customer_Joined %>% skim()
```

## Continious Variables

As it can be seen below our numeric variables don't really explain any variation(all smoother lines are horizontal.).Those variables will most likely be useless in the modelling process.
 
```{r}
plot_scatter <- function(x){
  
    ggplot(aes_string(x = x, y = "profit_per_transaction"), data = Customer_Joined)+
    geom_point(alpha = 0.8)+
    geom_smooth()
}

vars_to_plot <- c("tenure", "age", "past_3_years_bike_related_purchases",
                  "property_valuation")

patchwork::wrap_plots(map(vars_to_plot, plot_scatter))

```


## Categorical Variables

I binned the post codes in the assumption that post codes that are closer each other numerically should also be closer geographically. Maybe some places are much more suitable for bike riding than others. Box-Plot below kind of proves this point. It should be noted that even though some places differ by their median values their distributions are similar. This presents problems in terms of Machine Learning modeling.

```{r, fig.width=7, fig.height=8}
Customer_Joined %>% 
  mutate(post_code_cut = cut_width(postcode, 100, dig.lab = 5)) %>% 
  ggplot(aes(profit_per_transaction, reorder(post_code_cut, profit_per_transaction, FUN = median)))+
  geom_boxplot()+
  geom_vline(aes(xintercept = median(profit_per_transaction)), color = "red", linewidth = 1.1)
```

Still, I am going to use post code ranges which diverged from the sample median. In order to be able to to that I divided group median with sample median and created dummy variables if the ratio is above or below 1.

```{r}
Customer_Joined %>% 
  mutate(post_code_cut = cut_width(postcode, 100, dig.lab = 5)) %>% 
  group_by(post_code_cut) %>% 
  summarise(
    median_profit = median(profit_per_transaction),
    sd_profit = sd(profit_per_transaction)
  ) %>% 
  mutate(median_ratio = median_profit/median(Customer_Joined$profit_per_transaction)) %>% 
  arrange(desc(median_ratio))

diverging_post_codes <- Customer_Joined %>% 
  mutate(post_code_cut = cut_width(postcode, 100, dig.lab = 5)) %>% 
  group_by(post_code_cut) %>% 
  summarise(
    median_profit = median(profit_per_transaction),
    sd_profit = sd(profit_per_transaction)
  ) %>% 
  mutate(median_ratio = median_profit/median(Customer_Joined$profit_per_transaction)) %>% 
  arrange(desc(median_ratio)) %>% 
  filter(median_ratio > 1.07 | median_ratio < 0.9) %>% 
  pull(post_code_cut)

diverging_post_codes <- as.character(diverging_post_codes)
```

```{r}
Customer_Joined <- Customer_Joined %>% 
  mutate(post_code_cut = as.character(cut_width(postcode, 100, dig.lab = 5))) %>% 
  mutate(diverging_post_code = ifelse(post_code_cut %in% diverging_post_codes, post_code_cut, "Other"))

New_Costumers <- New_Costumers %>% 
  mutate(post_code_cut = as.character(cut_width(postcode, 100, dig.lab = 5))) %>% 
  mutate(diverging_post_code = ifelse(post_code_cut %in% diverging_post_codes, post_code_cut, "Other"))
```

`job_industry_category` isn't showing justifiable differences so I am leaving it out.

```{r}
Customer_Joined %>% 
  ggplot(aes(profit_per_transaction, reorder(job_industry_category, profit_per_transaction, FUN = median)))+
  geom_boxplot()+
  geom_vline(aes(xintercept = median(profit_per_transaction)), color = "red", linewidth = 1.1, alpha = 0.4)+
  geom_vline(aes(xintercept = mean(profit_per_transaction)), color = "blue", linewidth = 1.1, alpha = 0.4)
```

Manager level customers are very lucrative. Of course having incomes of customers instead of their jobs would be better. It should be possible to get average income values for these professions from 3rd party databases but for now that's too much work.

```{r, fig.height=16, fig.width=8}
Customer_Joined %>% 
  ggplot(aes(profit_per_transaction, reorder(job_title, profit_per_transaction, FUN = median)))+
  geom_boxplot()+
  geom_vline(aes(xintercept = median(profit_per_transaction)), color = "red", linewidth = 1.1, alpha = 0.4)+
  geom_vline(aes(xintercept = mean(profit_per_transaction)), color = "blue", linewidth = 1.1, alpha = 0.4)
```


I am using the same median ratio trick to differentiate jobs and create dummies for those.

```{r}
diverging_job_titles <- Customer_Joined %>% 
  group_by(job_title) %>% 
  summarise(
    median_profit = median(profit_per_transaction),
    sd_profit = sd(profit_per_transaction)
  ) %>% 
  mutate(median_ratio = median_profit/median(Customer_Joined$profit_per_transaction)) %>% 
  arrange(desc(median_ratio)) %>% 
  filter(median_ratio > 1.2 | median_ratio < 0.85) %>% 
  pull(job_title)
```

```{r}
Customer_Joined <- Customer_Joined %>% 
  mutate(diverging_job_title = ifelse(job_title %in% diverging_job_titles, job_title, "Other")) 

New_Costumers <- New_Costumers %>% 
  mutate(diverging_job_title = ifelse(job_title %in% diverging_job_titles, job_title, "Other"))
```

Lastly I am saving cleaned tables. Machine Learning Modeling will be done in Python.

```{r}
#write_csv(Customer_Joined, "customer_all.csv")

#write_csv(New_Costumers, "new_customers.csv")
```




























